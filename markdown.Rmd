---
title: "Project1"
author: "Georges Ip"
date: "2/2/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Import libraries
```{r message=FALSE}
library(tidyverse)
library(tseries) 
library(car)
library(forecast) 
require(stats)
library(vars)
library(MASS) 
require(stats4) 
require("datasets") 
require(graphics) 
library(RColorBrewer) 
library(dynlm)
library(ggcorrplot)
library(olsrr)
library(readxl)
library(ISLR)
library(dplyr)
library(tidyverse)
library(class)
library(gridExtra)
library(caTools)
library(boot)
library(caret)
library(e1071)
library(Hmisc)
library(DMwR)
```


*Import datasets*
```{r}
train <- read_csv("train.csv")
test <- read_csv("test.csv")
```



*Explore dataset*
```{r}
attach(train)
head(train)
colSums(is.na(train))
```



There are quite a few missing observations in Age and Cabin, as well as 2 observations in Embarked. Some of the indicator variables are also classified as num. 



```{r}
#First, deal with the factor variables
factors <- c("Survived", "Pclass", "Sex", "Embarked")
train[,factors] <- lapply(train[,factors], factor)
```



Now we have convertd to factor variables, we can conduct some preliminary descriptive analysis. A correlation plot will be more difficult to interpret as we have a few factor variables. 
```{r}
#First look at Passenger Class
table1 <- table(Pclass,Survived)
ftable(round(prop.table(table1), 3))
```



From this table, we can see that there is quite a clear relationship between survival and Pclass, with a higher proportion of survivors in better passenger classes. 
```{r}
#Now look at Sex
table2 <- table(Sex, Survived)
ftable(table2)
```



It is evident that a higher proportion of females survived compared to males

```{r}
#Look at the combined table
table3 <- table(Pclass, Sex, Survived)
ftable(table3)
```


This table is slightly more diifficult to interpret, but ones notable takeaway is that both trends we identified within Pclass and Sex still hold. This could be variables used in classification. We now turn our attention to numerical variables.



```{r}
#Add a histogram to see if ages affects survival
hist(Age[Survived == 0], density = 50, col = 2, angle = 0)
hist(Age[Survived == 1], add=TRUE, density = 25, col = 3, angle = 0)
```



*Now we deal with NAs*
```{r}
#Construct submission .csv
#Look at number of NAs
colSums(is.na(train))
#Imputation of NAs using mean/mode
impute.train <- train
train.mr.log <- grepl("Mr.", train$Name)
train.mrs.log <- grepl("Mrs.", train$Name)
train.master.log <- grepl("Master", train$Name)
train.miss.log <- grepl("Miss", train$Name)

mr.age <- median(train$Age[train.mr.log], na.rm = T)
mrs.age <- median(train$Age[train.mrs.log], na.rm = T)
master.age <- median(train$Age[train.master.log], na.rm = T)
miss.age <- median(train$Age[train.miss.log], na.rm = T)

impute.train$Age[is.na(train$Age) & train.mr.log] <- mr.age
impute.train$Age[is.na(train$Age) & train.mrs.log] <- mrs.age
impute.train$Age[is.na(train$Age) & train.master.log] <- master.age
impute.train$Age[is.na(train$Age) & train.miss.log] <- miss.age

#Imputation of remaining NAs using kNN
impute.train$Pclass <- as.numeric(as.character(impute.train$Pclass))
impute.train$Sex <- as.numeric(impute.train$Sex)
impute.train$EmbC <- as.numeric(impute.train$Embarked == "C")
impute.train$EmbQ <- as.numeric(impute.train$Embarked == "Q")
impute.train$EmbS <- as.numeric(impute.train$Embarked == "S")
impute.train <- subset(impute.train, select = -c(Name, Ticket, Cabin, Embarked))
impute.train <- as.data.frame(impute.train)
impute.train <- knnImputation(impute.train)
colSums(is.na(impute.train))
```

```{r}
#Look at number of NAs
colSums(is.na(test))
#Imputation of NAs using mean/mode
impute.test <- test

test.mr.log <- grepl("Mr.", test$Name)
test.mrs.log <- grepl("Mrs.", test$Name)
test.master.log <- grepl("Master", test$Name)
test.miss.log <- grepl("Miss", test$Name)

impute.test$Age[is.na(test$Age) & test.mr.log] <- mr.age
impute.test$Age[is.na(test$Age) & test.mrs.log] <- mrs.age
impute.test$Age[is.na(test$Age) & test.master.log] <- master.age
impute.test$Age[is.na(test$Age) & test.miss.log] <- miss.age

#Imputation of NAs using kNN
impute.test$Pclass <- as.numeric(as.character(test$Pclass))
impute.test$Sex <- as.numeric(test$Sex)
impute.test$EmbC <- as.numeric(test$Embarked == "C")
impute.test$EmbQ <- as.numeric(test$Embarked == "Q")
impute.test$EmbS <- as.numeric(test$Embarked == "S")
impute.test <- subset(impute.test, select = -c(Name, Ticket, Cabin, Embarked))
impute.test <- as.data.frame(impute.test)
impute.test <- knnImputation(impute.test)
colSums(is.na(impute.test))
```

```{r}
#Subset the train set
set.seed(1)
train.vector = sample(1:891,500)
train.set = impute.train[train.vector,]
test.set = impute.train[-train.vector,]
#validation
validation.set = impute.train$Survived[-train.vector]
```


```{r}
#Clean test set as well and add a Survived column 
factors <- c("Pclass", "Sex", "Embarked")
test[,factors] <- lapply(test[,factors], factor)
```



```{r, warning=FALSE}
#Try LDA
lda.fit=lda(Survived~Sex + Age + Pclass + SibSp + Parch + Fare, data=train.set, na.action = na.omit)
lda.pred=predict(lda.fit, test.set)
lda.class=lda.pred$class
mean(lda.class==validation.set, na.rm = T)
table(lda.class ,validation.set)
```


Using LDA with all the variables results in an accuracy of 80.05%



```{r, warning=FALSE}
#Try LDA
lda.fit2=lda(Survived~Sex + Age + Pclass + SibSp, data=train.set, na.action = na.omit)
lda.pred2=predict(lda.fit2, test.set)
lda.class2=lda.pred2$class
mean(lda.class2==validation.set, na.rm = T)
table(lda.class2 ,validation.set)
```


Removing Parch and Fare using LDA results in an accuracy of 81.07%


```{r, warning=FALSE}
#testing QDA
qda.fit=qda(Survived~Sex + Age + Pclass + SibSp, data=train.set, na.action = na.omit)
qda.pred=predict(qda.fit, test.set)
qda.class=qda.pred$class
mean(qda.class==validation.set, na.rm = T)
table(qda.class ,validation.set)
```


Using QDA with these predictors increases our accuracy to 82.35%8


```{r}
#For the kNN model, we have to recode our factor variables as dummy variables with integer levels
#factors are "Survived", "Pclass", "Sex", "Embarked"
train.set.knn <- train.set

train.set.knn$Survived <- as.numeric(as.character(train.set$Survived))
train.set.knn$Pclass <- as.numeric(as.character(train.set$Pclass))
train.set.knn$Sex <- as.numeric(train.set$Sex)
#do for test set
test.set.knn <- test.set
#construct prediction vector
train.prediction.set <- train.set.knn$Survived
test.prediction.set <- test.set.knn$Survived
```

```{r}
#Try kNN model again
set.seed(123)
#try k = 1
knn.pred.1 = knn(train.set.knn, test.set.knn, train.prediction.set, k = 1)
mean(test.prediction.set != knn.pred.1)
table(knn.pred.1,test.prediction.set)
#try k = 3
knn.pred.3 = knn(train.set.knn, test.set.knn, train.prediction.set, k = 3)
mean(test.prediction.set != knn.pred.3)
table(knn.pred.3,test.prediction.set)
#try k = 5
knn.pred.5 = knn(train.set.knn, test.set.knn, train.prediction.set, k = 5)
mean(test.prediction.set != knn.pred.5)
table(knn.pred.5,test.prediction.set)
```



The accuracy of kNN is lower than our LDA and QDA methods. 

```{r}
#Construct submission .csv
#Create a dataframe
dummy <- test$PassengerId
submission <- as.data.frame(dummy)
#Implement QDA model
qda.fit.final=qda(Survived~Sex + Age + Pclass + SibSp, data=impute.train, na.action = na.omit)
#Predict the test dataframe
qda.pred.final=predict(qda.fit.final, impute.test)
qda.class.final=qda.pred.final$class
#Finish constructing submission df
submission <- mutate(submission, Survived = qda.class.final)
names(submission) <- c("PassengerId", "Survived")
#Create CSV
write.csv(submission, 'qda_submission.csv', row.names = FALSE)
```


Our final submission to Kaggle returned us a score of 0.7655. 
